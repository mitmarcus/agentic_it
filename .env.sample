# Authentication
EMAIL=<stibo_login>
PASSWORD=<stibo_password>
STIBO_STATUS_URL=<status_page_url>
JIRA_URL = "https://aisandbox.stibodx.com"
PROJECT_KEY = "AIS"
AIS_API_TOKEN=<jira_token>

# ChromaDB Configuration
CHROMADB_MODE=server
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000
CHROMADB_COLLECTION=it_support_docs

# LLM Configuration
# Provider: "openai", "azure", or "groq" (default: groq)
LLM_PROVIDER=groq
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024
LLM_MAX_RETRIES=3
LLM_RETRY_WAIT=2

# Azure OpenAI Configuration (if LLM_PROVIDER=azure)
AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>
AZURE_OPENAI_ENDPOINT=<https://your-azure-openai-endpoint.openai.azure.com/>
AZURE_OPENAI_DEPLOYMENT=<your_deployment_name>

# Groq Configuration (if LLM_PROVIDER=groq)
GROQ_API_KEY=<your_groq_api_key>
GROQ_MODEL=llama-3.3-70b-versatile

# Embedding Configuration
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384
EMBEDDING_DEVICE=cpu
EMBEDDING_NORMALIZE=true
EMBEDDING_BATCH_SIZE=32

# RAG Configuration
# Optimized: Increased top_k from 3 to 5 for better coverage
RAG_TOP_K=5
# Optimized: Adjusted threshold from 0.6/0.7 to 0.65 for balance between precision and recall
RAG_MIN_SCORE=0.65
RAG_MAX_CONTEXT_TOKENS=2000

# Agent Decision Making
# Lower threshold means agent will answer more readily when it has context
AGENT_CLARIFY_CONFIDENCE_THRESHOLD=0.7
AGENT_DOC_CONFIDENCE_THRESHOLD=0.6
AGENT_RATE_LIMIT_ANSWER_CONFIDENCE=0.5
AGENT_SYSTEM_ERROR_CONFIDENCE=0.3
AGENT_MAX_TURNS=5

# Document Ingestion
INGESTION_SOURCE_DIR=./data/docs
# Optimized: Increased from 500 to 800 for better semantic coherence
INGESTION_CHUNK_SIZE=800
# Optimized: Increased from 50 to 100 for better context continuity between chunks
INGESTION_CHUNK_OVERLAP=100
INGESTION_BATCH_SIZE=10

# Agent Configuration
AGENT_MAX_TURNS=5
AGENT_ANSWER_CONFIDENCE=0.8
AGENT_CLARIFY_CONFIDENCE=0.5

# Troubleshooting Configuration
TROUBLESHOOT_ESCALATE_FAILED_STEPS=3
TROUBLESHOOT_FALLBACK_FAILED_STEPS=2

# API Configuration
API_HOST=0.0.0.0
API_PORT=8080
CORS_ORIGIN_1=http://localhost:3000
CORS_ORIGIN_2=http://localhost:8080

# Logging
LOG_LEVEL=INFO
LOG_FILE_ENABLED=true
LOG_FILE_PATH=./logs/app.log
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

# Tracing with Langfuse (OPTIONAL)
LANGFUSE_SECRET_KEY=<your_langfuse_secret_key>
LANGFUSE_PUBLIC_KEY=<your_langfuse_public_key>
LANGFUSE_HOST=https://cloud.langfuse.com
TRACING_DEBUG=false
TRACE_INPUTS=true
TRACE_OUTPUTS=true
TRACE_PREP=false
TRACE_EXEC=true
TRACE_POST=false
TRACE_ERRORS=true

# Reranking Configuration
RERANK_ENABLED=true
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Query Expansion Configuration
QUERY_EXPANSION_ENABLED=false
# Warning: Enabling adds an LLM call per query, increasing latency and cost
HYDE_ENABLED=false
# HyDE (Hypothetical Document Embeddings) - generates hypothetical answer for embedding
# Even more expensive but can improve complex query retrieval

# Feedback Loop Configuration
FEEDBACK_ENABLED=true
FEEDBACK_STORAGE_PATH=./logs/feedback.jsonl
# Collects user feedback to identify improvement opportunities
